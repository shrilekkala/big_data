---
output:
  pdf_document: default
  html_document: default
---

# BUS 41201 Homework 4 Assignment

## Group 24: Shihan Ban, Yi Cao, Shri Lekkala, Ningxin Zhang

## 16 April 2024

### Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      fig.width = 6, fig.height = 4,
                      warning = FALSE,
                      cache = TRUE,
                      digits = 3,
                      width = 48) 
```

```{r}
## microfinance network 
## data from BANERJEE, CHANDRASEKHAR, DUFLO, JACKSON 2012

## data on 8622 households
hh <- read.csv("microfi_households.csv", row.names="hh")
hh$village <- factor(hh$village)

## We'll kick off with a bunch of network stuff.
## This will be covered in more detail in lecture 6.
## get igraph off of CRAN if you don't have it
## install.packages("igraph")
## this is a tool for network analysis
## (see http://igraph.sourceforge.net/)
library(igraph)
edges <- read.table("microfi_edges.txt", colClasses="character")
## edges holds connections between the household ids
hhnet <- graph.edgelist(as.matrix(edges))
hhnet <- as.undirected(hhnet) # two-way connections.

## igraph is all about plotting.  
V(hhnet) ## our 8000+ household vertices
## Each vertex (node) has some attributes, and we can add more.
V(hhnet)$village <- as.character(hh[V(hhnet),'village'])
## we'll color them by village membership
vilcol <- rainbow(nlevels(hh$village))
names(vilcol) <- levels(hh$village)
V(hhnet)$color = vilcol[V(hhnet)$village]
## drop HH labels from plot
V(hhnet)$label=NA

# graph plots try to force distances proportional to connectivity
# imagine nodes connected by elastic bands that you are pulling apart
# The graphs can take a very long time, but I've found
# edge.curved=FALSE speeds things up a lot.  Not sure why.

## we'll use induced.subgraph and plot a couple villages 
village1 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="1"))
village33 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="33"))

# vertex.size=3 is small.  default is 15
plot(village1, vertex.size=3, edge.curved=FALSE)
plot(village33, vertex.size=3, edge.curved=FALSE)
```

```{r}
library(gamlr)

## match id's; I call these 'zebras' because they are like crosswalks
zebra <- match(rownames(hh), V(hhnet)$name)

## calculate the `degree' of each hh: 
##  number of commerce/friend/family connections
degree <- degree(hhnet)[zebra]
names(degree) <- rownames(hh)
degree[is.na(degree)] <- 0 # unconnected houses, not in our graph

## if you run a full glm, it takes forever and is an overfit mess
# > summary(full <- glm(loan ~ degree + .^2, data=hh, family="binomial"))
# Warning messages:
# 1: glm.fit: algorithm did not converge 
# 2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
```

\newpage

## Question 1

### I’d transform degree to create our treatment variable d. What would you do and why?

We can first plot a histogram of the degree variable to get an idea of it's structure:

```{r}
hist(degree)
```

From the graph, it might be appropriate to perform a logarithmic transformation for the following reasons:

-   It appears that the degree frequency is highly skewed to the right as there are many nodes with few connections (degree \< 20), but few nodes with many connections (degree \> 40). So by taking a log transformation, we can normalize the distribution, making it more symmetric and more suitable to statistical analyses.

-   The histogram appears to follow an exponential / multiplicative relationship. So transforming the data logarithmically can make the relationship more linear, which is easier to model and interpret in regression models.

-   We can reduce the range of variability in degree values, effectively performing a dimensionality reduction. This is useful to prevent the model being overly effected by outliers, i.e. households with a very high number of connections.

```{r}
# Transform degree and add it to the hh dataset
hh$d = log1p(degree)
head(hh)
```

\newpage

## Question 2

### Build a model to predict d from x, our controls.

### Comment on how tight the fit is, and what that implies for estimation of a treatment effect.

```{r}
model_d <- lm(d ~ village + religion + roof + rooms + beds + electricity + ownership + leader, data = hh)
# Display model summary
summary(model_d)
plot(model_d)
```

In this model, some variables, like rooms, electricity, beds, and leader, are statistically significant, implying they have a notable impact on the response variable.

The model we've run seems reasonably well-specified based on the first plot but does show potential outliers.The presence of influential points, as suggested by the fourth plot, means that the estimated treatment effect may be driven by a small number of cases. Thus, while there are indicators of a decent model fit, the presence of outliers and influential points suggests that the treatment effect estimation may be sensitive to particular data points. When interpreting treatment effects, it's crucial to consider these factors, as they can affect the validity and generalizability of the findings.

------------------------------------------------------------------------

\newpage

```{r}
# control variables
x = model.matrix(d ~ village + religion + roof + rooms + beds + electricity + ownership + leader - 1, data = hh)  # -1 to omit intercept

# dependent variable
y = hh$loan

# treatment variable
d = hh$d

# Estimate d_hat with lasso regression of d on x.
treat = gamlr(x, d, lambda.min.ratio=1e-4)

# Isolate dhat (the part of treatment that we can predict with x's)
d_hat = predict(treat, x, type="response")

# Plot d_hat against d
plot(d_hat, d)
```

To assess the model fit, we can compute the in sample R2

```{r}
# In-sample R2
cor(drop(d_hat), d)^2
```

So the in-sample R2 value suggests that $\approx 8.19$ % of the variance in d is explained by the control variables. Thus the model does not have a tight fit, and this implies that there may be other confounding variables not included in the model that account for d.

Thus the predictive power of our model is limited due to the large percentage of unexplained variance in d. And further analyses may lead to less accurate and biased estimates, so our confidence in a estimating treatment effect would be low.

\newpage

## Question 3

### Use predictions from [2] in an estimator for effect of d on loan.

```{r}
# Second Stage Lasso

# Do a lasso of y on [d, d_hat, x], with d_hat unpenalized
causal = gamlr(cbind(d, d_hat, x), y, free=2)

# Second 
print(coef(causal)["d",])
```

Using the two-stage lasso process, we find the best predictor for y from d and x after the influence of d_hat is removed.

We observe that the coefficient of the log transformed degree variable is 0.0187176, which suggests that there is a small positive relationship between the degree of connectivity and the likelihood of adopting a loan.

```{r}
exp(coef(causal)["d",])
```

By taking the exponential of the coefficient, we compute the odds ratio between degree and loan. That is, a one unit increase in the log transformed degree of connection corresponds to an $\approx 1.89$ % increase in the probability of a household taking a loan.

\newpage

## Question 4

### Compare the results from [3] to those from a straight (naive) lasso for loan on d and x.

```{r}
# Compute a naive lasso for loan
# We use binomial here since we know y (loan) is in [0,1]
naive = gamlr(cbind(d, x), y, family="binomial")

# Compare naive and 2-stage lasso
cat("The coefficient for d from the naive lasso is:", coef(naive)["d",], "\n")
cat("The coefficient for d from the causal lasso is:", coef(causal)["d",])
```

```{r}
exp(coef(naive)["d",])
```

### Explain why they are similar or different.

Firstly we observe that the coefficient for `d` from the naive model is approximately an order of 10 greater than the one from the causal model. That is, a one unit increase in log transformed degree would suggest there would be a an $\approx 16.9$ % increase in the probability of a household taking a loan (compared to $\approx 1.89$ % from Q3).\

They are different as the naive model does not separate the treatment and the control variables, but rather uses them all as independent variables in the regression. This may result in the coefficient for `d` having contributions from confounding variables which are not accounted for, and thus indicates a much more significant effect than the causal model.

In comparison, the causal model involved a 2-stage LASSO process by incorporating **`d_hat`**, the predicted values of **`d`** based on **`x`**. This means that this model controlled for the portion of **`d`** that could be predicted from the control variables, aiming to isolate the more variation in d. (However there could still be effects from confounding variables that were not in the data set).

So the two-stage Lasso model provides a more conservative but likely more accurate estimate by explicitly modeling and removing the predictable part of **`d`** based on the observed covariates **`x`**.

\newpage

## Question 5

### Bootstrap your estimator from [3] and describe the uncertainty.

\newpage

## [+]

### Can you think of how you’d design an experiment to estimate the treatment effect of network degree?
