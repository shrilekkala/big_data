---
output:
  pdf_document: default
  html_document: default
---

# BUS 41201 Homework 4 Assignment

## Group 24: Shihan Ban, Yi Cao, Shri Lekkala, Ningxin Zhang

## 16 April 2024

### Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      fig.width = 6, fig.height = 4,
                      warning = FALSE,
                      cache = TRUE,
                      digits = 3,
                      width = 48) 
set.seed(123)
```

```{r}
## microfinance network 
## data from BANERJEE, CHANDRASEKHAR, DUFLO, JACKSON 2012

## data on 8622 households
hh <- read.csv("microfi_households.csv", row.names="hh")
hh$village <- factor(hh$village)

## We'll kick off with a bunch of network stuff.
## This will be covered in more detail in lecture 6.
## get igraph off of CRAN if you don't have it
## install.packages("igraph")
## this is a tool for network analysis
## (see http://igraph.sourceforge.net/)
library(igraph)
edges <- read.table("microfi_edges.txt", colClasses="character")
## edges holds connections between the household ids
hhnet <- graph.edgelist(as.matrix(edges))
hhnet <- as.undirected(hhnet) # two-way connections.

## igraph is all about plotting.  
V(hhnet) ## our 8000+ household vertices
## Each vertex (node) has some attributes, and we can add more.
V(hhnet)$village <- as.character(hh[V(hhnet),'village'])
## we'll color them by village membership
vilcol <- rainbow(nlevels(hh$village))
names(vilcol) <- levels(hh$village)
V(hhnet)$color = vilcol[V(hhnet)$village]
## drop HH labels from plot
V(hhnet)$label=NA

# graph plots try to force distances proportional to connectivity
# imagine nodes connected by elastic bands that you are pulling apart
# The graphs can take a very long time, but I've found
# edge.curved=FALSE speeds things up a lot.  Not sure why.

## we'll use induced.subgraph and plot a couple villages 
village1 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="1"))
village33 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="33"))

# vertex.size=3 is small.  default is 15
plot(village1, vertex.size=3, edge.curved=FALSE)
plot(village33, vertex.size=3, edge.curved=FALSE)
```

```{r}
library(gamlr)

## match id's; I call these 'zebras' because they are like crosswalks
zebra <- match(rownames(hh), V(hhnet)$name)

## calculate the `degree' of each hh: 
##  number of commerce/friend/family connections
degree <- degree(hhnet)[zebra]
names(degree) <- rownames(hh)
degree[is.na(degree)] <- 0 # unconnected houses, not in our graph

## if you run a full glm, it takes forever and is an overfit mess
# > summary(full <- glm(loan ~ degree + .^2, data=hh, family="binomial"))
# Warning messages:
# 1: glm.fit: algorithm did not converge 
# 2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
```

\newpage

## Question 1

### I’d transform degree to create our treatment variable d. What would you do and why?

We can first plot a histogram of the degree variable to get an idea of it's structure:

```{r}
hist(degree)
```

From the graph, it might be appropriate to perform a logarithmic transformation for the following reasons:

-   It appears that the degree frequency is highly skewed to the right as there are many nodes with few connections (degree \< 20), but few nodes with many connections (degree \> 40). So by taking a log transformation, we can normalize the distribution, making it more symmetric and more suitable to statistical analyses.

-   The histogram appears to follow an exponential / multiplicative relationship. So transforming the data logarithmically can make the relationship more linear, which is easier to model and interpret in regression models.

-   We can reduce the range of variability in degree values, effectively performing a dimensionality reduction. This is useful to prevent the model being overly effected by outliers, i.e. households with a very high number of connections.

```{r}
# Transform degree and add it to the hh dataset
hh$d = log1p(degree)
head(hh)
```

\newpage

## Question 2

### Build a model to predict d from x, our controls.

```{r}
# control variables
x = model.matrix(d ~ village + religion + roof + rooms + beds + electricity + ownership + leader - 1, data = hh)  # -1 to omit intercept

# dependent variable
y = hh$loan

# treatment variable
d = hh$d

# Estimate d_hat with lasso regression of d on x.
treat = gamlr(x, d, lambda.min.ratio=1e-4)

# Isolate dhat (the part of treatment that we can predict with x's)
d_hat = predict(treat, x, type="response")

# Plot d_hat against d
plot(d_hat, d)
```

\newpage

### Comment on how tight the fit is, and what that implies for estimation of a treatment effect.

To assess the model fit, we can compute the in sample R2

```{r}
# In-sample R2
cor(drop(d_hat), d)^2
```

So the in-sample R2 value suggests that $\approx 8.19$ % of the variance in d is explained by the control variables. Thus the model does not have a tight fit, and this implies that there may be other confounding variables not included in the model that account for d.

Thus the predictive power of our model is limited due to the large percentage of unexplained variance in d. And further analyses may lead to less accurate and biased estimates, so our confidence in a estimating treatment effect would be low.

\newpage

## Question 3

### Use predictions from [2] in an estimator for effect of d on loan.

```{r}
# Second Stage Lasso

# Do a lasso of y on [d, d_hat, x], with d_hat unpenalized
causal = gamlr(cbind(d, d_hat, x), y, free=2)

# Second 
print(coef(causal)["d",])
```

Using the two-stage lasso process, we find the best predictor for y from d and x after the influence of d_hat is removed.

We observe that the coefficient of the log transformed degree variable is 0.0187176, which suggests that there is a small positive relationship between the degree of connectivity and the likelihood of adopting a loan.

```{r}
exp(coef(causal)["d",])
```

By taking the exponential of the coefficient, we compute the odds ratio between degree and loan. That is, a one unit increase in the log transformed degree of connection corresponds to an $\approx 1.89$ % increase in the probability of a household taking a loan.

\newpage

## Question 4

### Compare the results from [3] to those from a straight (naive) lasso for loan on d and x.

```{r}
# Compute a naive lasso for loan
# We use binomial here since we know y (loan) is in [0,1]
naive = gamlr(cbind(d, x), y, family="binomial")

# Compare naive and 2-stage lasso
cat("The coefficient for d from the naive lasso is:", coef(naive)["d",], "\n")
cat("The coefficient for d from the causal lasso is:", coef(causal)["d",])
```

```{r}
exp(coef(naive)["d",])
```

### Explain why they are similar or different.

Firstly we observe that the coefficient for `d` from the naive model is approximately an order of 10 greater than the one from the causal model. That is, a one unit increase in log transformed degree would suggest there would be a an $\approx 16.9$ % increase in the probability of a household taking a loan (compared to $\approx 1.89$ % from Q3).\

They are different as the naive model does not separate the treatment and the control variables, but rather uses them all as independent variables in the regression. This may result in the coefficient for `d` having contributions from confounding variables which are not accounted for, and thus indicates a much more significant effect than the causal model.

In comparison, the causal model involved a 2-stage LASSO process by incorporating **`d_hat`**, the predicted values of **`d`** based on **`x`**. This means that this model controlled for the portion of **`d`** that could be predicted from the control variables, aiming to isolate the more variation in d. (However there could still be effects from confounding variables that were not in the data set).

So the two-stage Lasso model provides a more conservative but likely more accurate estimate by explicitly modeling and removing the predictable part of **`d`** based on the observed covariates **`x`**.

\newpage

## Question 5

### Bootstrap your estimator from [3] and describe the uncertainty.

```{r}

## BOOTSTRAP 
n <- nrow(x)

gamb = c() # empty gamma

for(b in 1:50){
	## create a matrix of resampled indices
	ib = sample(1:n, n, replace=TRUE)

	## create the resampled data
	xb = x[ib,]
	db = d[ib]
	yb = y[ib]

	## run the treatment regression
	treatb = gamlr(xb,db,lambda.min.ratio=1e-3)
	dhatb = predict(treatb, xb, type="response")
	fitb = gamlr(cbind(db,dhatb,xb),yb,free=2)
	gamb = c(gamb,coef(fitb)["db",])
}
```

After running the bootstrap 50 times, we get the following summary statistics for the estimates:

```{r}
summary(gamb)
```

We can also plot a histogram to observe the variability in the estimates, with a red line for our original non-bootstrapped estimate:

```{r}
hist(gamb)
abline(v=coef(causal)["d",], col=2)
```

We also plot a box plot and note the standard deviation:

```{r}
boxplot(gamb)
```

```{r}
cat("The standard deviation in estimates is: ", sd(gamb))
```

So after running the bootstrap 50 times, we observe that the median estimate is $\approx 0.0181$, with a standard deviation of $\approx 0.005$. So the bootstrap estimates seem to have fairly low variability which suggests that the estimate obtained this way is stable.

From the box-plot, we observe that the interquartile range is fairly narrow, which suggests that the estimates are centered around the median, and also suggests that the model prediction is robust.

Finally the histogram shows that the estimates seem to form a symmetric distribution, centered around the median. Further the original estimate from the full model is close to the center of the bootstrapped estimates which suggests that the original estimate was also robust.

\newpage

## [Bonus]

### Can you think of how you’d design an experiment to estimate the treatment effect of network degree?

In order to design such as experiment, one would ideally create a randomized experiment where one randomly selected group of individuals would increase their network degree, and the others would not.

However, this is easier said than done, as in real life increasing one's connections or network is not as simple as just asking someone to do so, and such processes take considerable time. However, one possible alternative is to randomly assign certain households to receive targeted opportunities and encouragement to increase their network and social connections. This could be in the form of community events, participation in social groups, or general networking opportunities. The other households would not receive such opportunities and would be left as a control group.

Following the experiment, we could then remeasure the outcome of interest (loan uptake) and compare the outcomes between the two groups. However, one would have to make sure that the control group's network stayed similar and the target group's network did increase before forming any conclusions.

However, there a few concerns regarding such an experiment. Firstly there could be spillover effects, where the treatment affects the control group as there might be mutual connections between the two groups. In such cases it would be harder to measure the treatment effect. Further, there might be also ethical concerns that the experiment we perform is manipulating human relationships and interactions, which is a topic that requires some sensitivity. In addition, some may view it as unfair if some people in a village get more networking opportunities and encouragement whilst others do not have the same access.
