---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
##########################################
# Setup
##########################################

knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 4,
	fig.width = 6,
	warning = FALSE,
	cache = TRUE,
	digits = 3,
	width = 48
)
 
# Required Packages
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(grid)
library(gridExtra)
library(RColorBrewer)
library(kableExtra)
library(gamlr)
library(bestNormalize)
library(tree)
library(janitor)
library(randomForest)
```

## 3. Dataset

### a) Understanding the data
```{r include=FALSE}
##########################################
# 3. a) Understanding the datasets
##########################################
# Load the datasets
googleplaystore_raw <- read.csv("data/googleplaystore.csv")
googleplaystore_user_reviews_raw <- read.csv("data/googleplaystore_user_reviews.csv")

# Check the column names
colnames(googleplaystore_raw)
colnames(googleplaystore_user_reviews_raw)

# Check the dimensions
dim(googleplaystore_raw)
dim(googleplaystore_user_reviews_raw)
```

### b) Data Cleaning
```{r include=FALSE}
##########################################
# 3. b) Data Cleaning
##########################################

# Convert the variables to the appropriate data type
googleplaystore <- googleplaystore_raw |>
  mutate(
    # Transform Installs and size to numeric
    Installs = gsub("\\+", "", as.character(Installs)),
    Installs = as.numeric(gsub(",", "", Installs)),
    Size = gsub("M", "", Size),
    # Convert apps with size < 1MB to 0, and transform to numeric
    Size = ifelse(grepl("k", Size), 0, as.numeric(Size)),
    # Transform reviews to numeric
    Reviews = as.numeric(Reviews),
    # Change currency numeric
    Price = as.numeric(gsub("\\$", "", as.character(Price))),
    # Convert Last.Updated to date
    Last.Updated = mdy(Last.Updated),
    # Change version number to 1 decimal, and add NAs where appropriate
    Android.Ver = gsub("Varies with device", NaN, Android.Ver),
    Android.Ver = as.numeric(substr(Android.Ver, start = 1, stop = 3)),
    Current.Ver = gsub("Varies with device", NaN, Current.Ver),
    Current.Ver = as.numeric(substr(Current.Ver, start = 1, stop = 3)),
  ) |>
  # Remove apps with Type 0 or NA
  filter(Type %in% c("Free", "Paid")) |>
  # Convert Category, Type, Content.Rating and Genres to factors
  mutate(
    App = as.factor(App),
    Category = as.factor(Category),
    Type = as.factor(Type),
    Content.Rating = as.factor(Content.Rating),
    Genres = as.factor(Genres)
  ) |>
  # Remove duplicate rows
  distinct()

```

\newpage
## 6. What are the key features that influence an app's rating?
App developers and key stakeholders often have an app's rating as an objective as this influences public perception of the app as well as attracting possible new users. So we decided to investigate our ability to predict an app's rating.

### A. Data Preparation and Initial Investigation

```{r}
# Data Processing
# Filter out rows where Rating is NaN
ratings_data <- googleplaystore |> na.omit()

# The data is scraped from August 2018
# Create a feature called Days Since Last Update
ratings_data$Days_Since_Update <- as.numeric(as.Date("2018-08-15") - ratings_data$Last.Updated)

# Log transform Installs, Size, and Reviews to remove skewness
ratings_data <- ratings_data |>
  mutate(log_Installs = log(ratings_data$Installs),
         normalized_Size = bestNormalize(ratings_data$Size)$x.t,
         log_Reviews = log(ratings_data$Reviews))

# Create dummy variables using model.matrix
x <- model.matrix(Rating ~ log_Reviews + normalized_Size + log_Installs + Price + Days_Since_Update + Category + Type + Content.Rating + Genres - 1, data = ratings_data)
# Response variable
y <- ratings_data$Rating  
```

Firstly, we process the data by eliminating any rows where the rating is "NaN".      
Secondly, we decided to create a new feature called "Days_Since_Update" as this may be more useful than just a specific date of update, and has the added advantage of being a numeric value. The original data was scraped in August 2018, with the latest update for an app being 8th August 2018. The original dataset had no specific day from which it was scraped, so we decided to use 15th August 2018 as an intermediary value, and calculated the different between this date and the "Last.Updated" to create the new feature.  
Finally, as we saw in the EDA, some of the numeric variables exhibited skewness, so we log transform Installs and Reviews, and normalize Reviews appropriately (we don't use a log transform here as some apps are listed as size 0).


In addition we also pruned our features to ignore "App" (as app names have added value for our modelling purposes),
"Current.Ver" and "Android.Ver" (as again one would assume that these versions hold no significant value on ratings).
Out of the remaining categorical variables (Category, Type, Content.Rating, and Genres), we convert these to dummy variables using `model.matrix` which can then be fed into the models.

This subsetted our data into 161 independent variables, and 1 dependent variable (the rating score).

```{r, fig.height=3.5, fig.align='center'}
ggplot(ratings_data, aes(x = Rating)) +
  geom_histogram(bins = 20, fill = "darkred") +
  ggtitle("Histogram of Ratings") +
  theme_minimal()
```

By looking at the histogram of the ratings, it is interesting to observe that the greatest density of ratings is between 4 and 5, which suggests that most users tend to leave a positive rating. Surprisingly the distribution is very heavily skewed to the left, so there are very few ratings close to 1.

It will be interesting to observe if our models are better at predicting high ratings correctly compared to low ratings.

### B. Analysis

#### LASSO

We first fit a LASSO model using x and y prepared above, and select a lambda using the AICc.

```{r, include = TRUE, fig.height=4, fig.align='center'}
# Fit the LASSO model using gamlr
set.seed(1024)
lasso_model <- gamlr(x, y, lambda.min.ratio=1e-3)
plot(lasso_model)
```

Above we can see the regularization paths for the penalized $\beta$ and the minimum AICc selection marked.

```{r}
# Find the index with lowest AICc
summary_output = summary(lasso_model)
best_aicc_index <- which.min(summary_output$aicc)

coefficients_lasso <- lasso_model$beta[, best_aicc_index]
highest_coefs <- head(sort(coefficients_lasso, decreasing = TRUE), 5)
lowest_coefs <- head(sort(coefficients_lasso, decreasing = FALSE), 5)

# Convert them to data frames
highest_df = data.frame(Feature = names(highest_coefs), Coefficient = highest_coefs, Impact = "Positive")
lowest_df = data.frame(Feature = names(lowest_coefs), Coefficient = lowest_coefs, Impact = "Negative")
coefficients_df <- rbind(highest_df, lowest_df)

# Ordering the dataframe by coefficient magnitude for clearer interpretation
coefficients_df <- coefficients_df[order(coefficients_df$Coefficient, decreasing=TRUE),]
kable(coefficients_df,
      caption = "Highest and Lowest Coefficients from LASSO Model",
      row.names = FALSE) |>
  kable_styling(bootstrap_options = c("striped", "hover"))
```

From looking at the coefficients with the largest positive and negative values, we observe that apps with the Genres: Board;Pretend Play, Comics;Creativity, Education;Creativity,all have strong positive association with higher ratings. This could suggest that apps with combined genres, and ones that promote creativity, play, and fun tend to be well-received by users and are could provide higher user-satisfaction

Interestingly, while the genre "education" seems to have a positive impact on rating , the genre "educational" appears to have the most negative impact in our model. This might suggest that apps related to education are polarizing to users, and they tend to either be satisfied and leave a high rating, or otherwise they do not meet user expectations and negatively influence the rating.

Also, another category of note is "Dating" which also has a large negative impact, and again this might be due to user dissatisfaction with the service, and makes sense considering the complicated and competitive nature of such apps.

Finally, the number of installs also seems to be a feature of note (log_Installs) as it somewhat counter-intuitively negatively impacts ratings. This might be explained by the idea that more popular apps are used by a broader audience with diverse expectations, and thus receive more critical reviews.

```{r, include = TRUE}
# Find the R2 of the lowest AICc slice
best_r2 <- summary_output$r2[best_aicc_index]
print(paste("In-sample R^2:", best_r2))
```
The in-sample R2 for the AICc slice of the LASSO path is $\approx$ 0.16, which suggests that this model is not a very good fit for our data and only about 16% of the variance in app ratings is explained by our predictors. This might suggest that the relationship between the features and ratings is not linear and more complex than this model can capture. So, next we aim to look at non linear models such as decision trees and random forests.

#### Decision Tree
We build a regression tree model using all the features from above.
```{r, fig.width=10, fig.align='center', fig.height=6}
# use names without spaces for tree package
x = clean_names(as.data.frame(x))
tree_model <- tree(y ~ ., data=x, mindev=0.00001)

plot(tree_model)
```

```{r}
evaluate_tree_model <- function(model, x, y) {
    # Predict ratings using the tree model
    predictions <- predict(model, x)
    
    # Calculate SSR (Sum of Squares of Residuals)
    SSR <- sum((y - predictions)^2)
    
    # Calculate SST (Total Sum of Squares)
    mean_rating <- mean(y)
    SST <- sum((y - mean_rating)^2)
    
    # Compute R^2
    R_squared <- 1 - (SSR / SST)
    
    # Calculate Mean Squared Error
    mse <- mean((y - predictions)^2)
    
    # Print the results
    cat("In-sample R^2:", R_squared, "\n")
    cat("Mean Squared Error:", mse, "\n")
}

evaluate_tree_model(tree_model, x, y)
```

The dendogram for the chosen tree is as above, with labels of variables left out for clarity of visualization.  
Looking at the $R^2$ score and mean square error, it is clear that the tree performs very well, however given that there are 1126 terminal nodes, it is very likely that overfitting is occuring, and the tree might not perform well out of sample.  
To address this, we prune the tree using cross-validation:

```{r, fig.align='center'}
# Cross Validation
cv_tree_model <- cv.tree(tree_model, K=50)

# Find the last index corresponding to minimum deviance
tree_index = max(which(cv_tree_model$dev == min(cv_tree_model$dev)))

# Find the tree size
tree_size = cv_tree_model$size[tree_index]

# Prune the tree
pruned_tree <- prune.tree(tree_model, best=tree_size)
plot(pruned_tree)
text(pruned_tree, pretty = 1)
```
The deviance for the cross-validated trees was minimal and the same for the number of leaves = 3 onwards.  
So, choosing this as our best tree size, we obtain the tree as above. This has the added advantage of being more interpretable, as now the predictions only depend on two variables: Reviews and Installs.

It is interesting to note that having more installs does not necessarily lead to a higher rating. As we see that in the left branch, having log_installs < 5.40989 (equivalent to a threshold of < 223.607 installs), leads to a higher predicted rating than otherwise. This aligns with our finding in the LASSO model above where log_installs had a significant negative coefficient.

Finally we also note that all the leaves of the pruned tree lie between 4.0 and 4.5, and this makes sense as this is where most of the ratings in our dataset lie (as seen on the histogram above). However this may mean that this model does not perform well when predicting lower ratings.

```{r}
evaluate_tree_model(pruned_tree, x, y)
```

The in-sample $R^2$ shows that when pruning the tree, we have sacrificed some explanation in variability. However the mean-squared error does not significantly increase which is a good sign.

```{r, fig.align='center'}
plot(x$log_reviews, x$log_installs, cex=exp(y)*.01,
     xlab = "Log Reviews", ylab = "Log Installs")
abline(v=8.65146, col=4, lwd=2)
lines(x=c(0,8.65146), y=c(5.40989,5.40990), col=4, lwd=2)
```
The pruned tree splits the data in the feature space as visualized above, where the point size is proportional to their rating.

Finally, in order to try and improve upon the tree model, we turn to random forests.

#### Random Forest

```{r}
tree_model <- tree(y ~ ., data=x, mindev=0.00001)

rf_model <- randomForest(y ~ ., data=x, importance = TRUE, do.trace = 1, ntree = 50)
save(rf_model, file = "model_data/rf_model.RData")
```


```{r, fig.align="center", fig.width=10, fig.height = 6}
kable(head(importance(rf_model), n = 10),
      caption = "Variable Importances from Random Forest",
      format = 'markdown') |>
  kable_styling(bootstrap_options = c("striped", "hover"))

varImpPlot(rf_model)
```

#### Evaluating all 3 models
```{r}
# Function to compute out of sample R2 for given models
compute_OOS_R2 <- function(model, test_x, test_y) {
    # Predict ratings using the model
    predictions <- predict(model, test_x)
    
    # Calculate SSR (Sum of Squares of Residuals)
    SSR <- sum((test_y - predictions)^2)
    
    # Calculate SST (Total Sum of Squares)
    mean_rating <- mean(test_y)
    SST <- sum((test_y - mean_rating)^2)
    
    # Compute R^2
    R_squared <- 1 - (SSR / SST)
    
    return (R_squared)
}

# Function to return a trained tree model
get_tree_model <- function(train_x, train_y) {
    # Intial Tree
    tree_model <- tree(y ~ ., data=x, mindev=0.00001)
    
    # Cross Validation
    cv_tree_model <- cv.tree(tree_model, K=50)
    
    # Find the last index corresponding to minimum deviance
    tree_index = max(which(cv_tree_model$dev == min(cv_tree_model$dev)))
    
    # Find the tree size
    tree_size = cv_tree_model$size[tree_index]
    
    # Prune the tree
    pruned_tree <- prune.tree(tree_model, best=tree_size)
    
    return(pruned_tree)
}
```

```{r}
# Initialize a data frame to store R-squared results
results_df <- data.frame(iteration = integer(),
                         R2_LASSO = numeric(),
                         R2_Tree = numeric(),
                         R2_rf = numeric())


# Loop for 20 iterations
for (i in 1:20) {
    # Progress Counter
    print(paste("Iteration: ", i, "/20"))

    # Randomly split data into training and testing sets
    n <- length(y)
    split <- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(0.8, 0.2))
    x_train <- x[split, ]
    x_test <- x[!split, ]
    y_train <- y[split]
    y_test <- y[!split]

    # Fit models on the training data
    print("Training LASSO")
    model_lasso <- gamlr(x_train, y_train, lambda.min.ratio=1e-3)
    print("Training Tree")
    model_tree <- get_tree_model(x_train, y_train)
    print("Training Random Forest")
    model_rf <- randomForest(y_train ~ ., data=x_train, ntree=50)

    # Compute out-of-sample R2 for each model
    R2_LASSO <- compute_OOS_R2(model_lasso, x_test, y_test)
    R2_Tree <- compute_OOS_R2(model_tree, x_test, y_test)
    R2_rf <- compute_OOS_R2(model_rf, x_test, y_test)

    # Store the results in the dataframe
    results_df <- rbind(results_df,
                        data.frame(iteration = i,
                                   R2_LASSO = R2_LASSO,
                                   R2_Tree = R2_Tree,
                                   R2_rf = R2_rf))
}
```
```{r}
save(results_df, file = "model_data/results_df.RData")
```


```{r}
# Plot the results
results_long <- pivot_longer(results_df,
                             cols = c(R2_LASSO, R2_Tree, R2_rf),
                             names_to = "Model",
                             values_to = "R2")
results_long$Model <- factor(results_long$Model, levels = c("R2_LASSO", "R2_Tree", "R2_rf"))


# Create a boxplot to compare the R2 scores for each model
ggplot(results_long, aes(x = Model, y = R2, fill = Model)) +
    geom_boxplot(fill = "darkred") +
    labs(title = "Comparison of Out-of-Sample R^2 Across Models",
         x = "Model",
         y = "Out-of-Sample R^2") +
    theme_minimal() +
    scale_x_discrete(labels = c("LASSO", "Pruned Tree", "Random Forest"))
    theme(legend.title = element_blank(),  # Remove the legend title
          legend.position = "none")
```



### C. Conclusion

\newpage
## 7. How does user sentiment in reviews correlate with app ratings?
### A. Introduction
### B. Analysis
#### Model 1.
#### Model 2.
#### Model 3.
### C. Conclusion

\newpage
## 8. Conclusion

\newpage
## 9. Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
