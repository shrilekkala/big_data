---
output:
  pdf_document: default
  html_document: default
---

# BUS 41201 Homework 2 Assignment

## Shihan Ban, Yi Cao, Shri Lekkala, Ningxin Zhang

## 2 April 2024

### Setup

```{r}
library(knitr) # library for markdown output
# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)

##### ******** Mortgage and Home Sales Data ******** #####

## Read in the data

homes <- read.csv("homes2004.csv")

# conditional vs marginal value


par(mfrow=c(1,2)) # 1 row, 2 columns of plots 

hist(homes$VALUE, col="grey", xlab="home value", main="")

plot(VALUE ~ factor(BATHS), 
    col=rainbow(8), data=homes[homes$BATHS<8,],
    xlab="number of bathrooms", ylab="home value")


# create a var for down payment being greater than 20%
homes$gt20dwn <- factor(0.2<(homes$LPRICE-homes$AMMORT)/homes$LPRICE)
```

```{r}
# You can try some quick plots.  Do more to build your intuition!

par(mfrow=c(1,2))
plot(VALUE ~ factor(STATE), data=homes,
     col=rainbow(nlevels(factor(homes$STATE))),
     ylim=c(0,10^6), cex.axis=.65)
plot(gt20dwn ~ factor(FRSTHO), data=homes,
     col=c(1,3), 
     xlab="Buyer's First Home?",
     ylab="Greater than 20% down")
```

### Question 1

Regress log price onto all variables but mortgage.\
What is the R2? How many coefficients are used in this model and how many are significant at 10% FDR?\
Re-run regression with only the significant covariates, and compare R2 to the full model. (2 points)

```{r}
# First convert all non-numeric columns in 'homes' to factors
homes = lapply(homes, function(x) if(!is.numeric(x)) factor(x) else x)

# Convert 'homes' back to a data frame, as lapply returns a list
homes = as.data.frame(homes)
```

```{r}
# regress log(PRICE) on everything except AMMORT 
pricey <- glm(log(LPRICE) ~ .-AMMORT, data=homes)

# Extract R-squared value the summary
summary_pricey <- summary(pricey)
R2_reduced = 1 - summary_pricey$deviance / summary_pricey$null.deviance
R2_reduced
```

So the R2 score is 0.4565419.

```{r}
# extract pvalues
pvals <- summary(pricey)$coef[-1,4]
length(pvals)
```

So there are 42 coefficients in this model.

```{r}
# Find the p-value cutoff at the 10% FDR level

# To find the p-value cut off we first order the p values
pvals_ordered <- pvals[order(pvals, decreasing=F)]

# Next we use the function fdr_cut function defined in class class to find the cutoff at level 0.1
fdr_cut <- function(pvals, q){
  pvals <- pvals[!is.na(pvals)]
  N <- length(pvals)
  k <- rank(pvals, ties.method="min")
  alpha <- max(pvals[ pvals<= (q*k/N) ])
  return(alpha)
}

p_cutoff = fdr_cut(pvals_ordered, q=0.1)
p_cutoff

# Find the number of significant coefficients at this level
sum(pvals < p_cutoff)
```

So out of the 42 coefficients, 36 are significant at the 10% FDR level.

```{r}
# Extract significant coefficients
significant_covariates = names(pvals)[pvals < p_cutoff]
significant_covariates
```

As there are covariates that correspond to factors, we extract only the relevant variable names and use them for our reduced model.

```{r}
# Get the names of significant variables in the dataset
significant_vars = c("EAPTBL", "ECOM2", "EGREEN", "EJUNK", "ELOW1", "ESFD", "EABAN", "HOWH", "HOWN", "ODORA", "STRNA", "ZINC2", "PER", "ZADULT", "HHGRAD", "INTW", "METRO", "STATE", "BATHS", "MATBUY", "DWNPAY", "VALUE", "FRSTHO", "gt20dwn")

# Construct the formula for the reduced model
reduced_formula_str = paste("log(LPRICE)", "~", paste(significant_vars, collapse = " + "))

# Rerun the regression with the significant covariates
reduced_model = glm(reduced_formula_str, data=homes)

# Extract R-squared value the summary
summary_reduced_model = summary(reduced_model)
R2_reduced = 1 - summary_reduced_model$deviance / summary_reduced_model$null.deviance
R2_reduced
```

So the R2 score for the reduced model is 0.4563139.

Which is slightly less than the R2 score of the full model (which was 0.4565419), this is expected as our reduced model has fewer covariates than the full model.

### Question 2

Fit a regression for whether the buyer had more than 20 percent down (onto everything but AMMORT and LPRICE).

Interpret effects for Pennsylvania state, 1st home buyers and the number of bathrooms.

Add and describe an interaction between 1st home-buyers and the number of baths. (2 points)

```{r}
# - don't forget family="binomial"!
# - use +A*B in formula to add A interacting with B
# Fit the logistic regression model excluding AMMORT and LPRICE
down_payment_model <- glm(gt20dwn ~ . -AMMORT -LPRICE + FRSTHO*BATHS, family="binomial", data=homes)

# Summary of the model to interpret coefficients
summary_down_payment_model <- summary(down_payment_model)

# Print the summary to interpret effects
summary_down_payment_model
```

The negative coefficient for the interaction term (BATHS:FRSTHOY) suggests that for first-time homebuyers, each additional bathroom decreases the log odds of putting down more than 20% by 0.202 compared to buyers who are not purchasing their first home. This could indicate that first-time home buyers are either purchasing less expensive homes with more bathrooms or that the presence of additional bathrooms diminishes their ability or inclination to make larger down payments, perhaps due to the overall higher costs associated with homes that have more bathrooms.

### Question 3

Focus only on a subset of homes worth $>100k$.\
Train the full model from Question 1 on this subset.\
Predict the left-out homes using this model.\
What is the out-of-sample fit (i.e. R2)?\
Explain why you get this value. (1 point)

```{r}
# this is your training sample
subset <- which(homes$VALUE>100000)

# Use the code ``deviance.R" to compute OOS deviance
source("deviance.R")

# Null model has just one mean parameter
ybar <- mean(log(homes$LPRICE[-subset]))
D0 <- deviance(y=log(homes$LPRICE[-subset]), pred=ybar)

```

```{r}
```
