---
output:
  pdf_document: default
  html_document: default
---

# BUS 41201 Homework 2 Assignment

## Shihan Ban, Yi Cao, Shri Lekkala, Ningxin Zhang

## 2 April 2024

### Setup

```{r}
library(knitr) # library for markdown output
# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)

##### ******** Mortgage and Home Sales Data ******** #####

## Read in the data

homes <- read.csv("homes2004.csv")

# conditional vs marginal value


par(mfrow=c(1,2)) # 1 row, 2 columns of plots 

hist(homes$VALUE, col="grey", xlab="home value", main="")

plot(VALUE ~ factor(BATHS), 
    col=rainbow(8), data=homes[homes$BATHS<8,],
    xlab="number of bathrooms", ylab="home value")


# create a var for down payment being greater than 20%
homes$gt20dwn <- factor(0.2<(homes$LPRICE-homes$AMMORT)/homes$LPRICE)
```

```{r}
# You can try some quick plots.  Do more to build your intuition!

par(mfrow=c(1,2))
plot(VALUE ~ factor(STATE), data=homes,
     col=rainbow(nlevels(factor(homes$STATE))),
     ylim=c(0,10^6), cex.axis=.65)
plot(gt20dwn ~ factor(FRSTHO), data=homes,
     col=c(1,3), 
     xlab="Buyer's First Home?",
     ylab="Greater than 20% down")
```

\newpage

### Question 1

#### Regress log price onto all variables but mortgage.

```{r}
# First convert all non-numeric columns in 'homes' to factors
homes = lapply(homes, function(x) if(!is.numeric(x)) factor(x) else x)

# Convert 'homes' back to a data frame, as lapply returns a list
homes = as.data.frame(homes)

# regress log(PRICE) on everything except AMMORT 
pricey <- glm(log(LPRICE) ~ .-AMMORT, data=homes)
```

#### What is the R2?

```{r}
# Extract R-squared value the summary
summary_pricey <- summary(pricey)
R2_reduced = 1 - summary_pricey$deviance / summary_pricey$null.deviance
R2_reduced
```

So the R2 score is 0.4565419.

#### How many coefficients are used in this model and how many are significant at 10% FDR?

```{r}
# extract pvalues
pvals <- summary(pricey)$coef[-1,4]
length(pvals)
```

So there are 42 coefficients in this model.

#### Re-run regression with only the significant covariates, and compare R2 to the full model. (2 points)

```{r}
# Find the p-value cutoff at the 10% FDR level

# To find the p-value cut off we first order the p values
pvals_ordered <- pvals[order(pvals, decreasing=F)]

# Next we use the function fdr_cut function defined in class class to find the cutoff at level 0.1
fdr_cut <- function(pvals, q){
  pvals <- pvals[!is.na(pvals)]
  N <- length(pvals)
  k <- rank(pvals, ties.method="min")
  alpha <- max(pvals[ pvals<= (q*k/N) ])
  return(alpha)
}

p_cutoff = fdr_cut(pvals_ordered, q=0.1)
p_cutoff

# Find the number of significant coefficients at this level
sum(pvals < p_cutoff)
```

So out of the 42 coefficients, 36 are significant at the 10% FDR level.

```{r}
# Extract significant coefficients
significant_covariates = names(pvals)[pvals < p_cutoff]
significant_covariates
```

As there are covariates that correspond to factors, we extract only the relevant variable names and use them for our reduced model.

```{r}
# Get the names of significant variables in the dataset
significant_vars = c("EAPTBL", "ECOM2", "EGREEN", "EJUNK", "ELOW1", "ESFD", "EABAN", "HOWH", "HOWN", "ODORA", "STRNA", "ZINC2", "PER", "ZADULT", "HHGRAD", "INTW", "METRO", "STATE", "BATHS", "MATBUY", "DWNPAY", "VALUE", "FRSTHO", "gt20dwn")

# Construct the formula for the reduced model
reduced_formula_str = paste("log(LPRICE)", "~", paste(significant_vars, collapse = " + "))

# Rerun the regression with the significant covariates
reduced_model = glm(reduced_formula_str, data=homes)

# Extract R-squared value the summary
summary_reduced_model = summary(reduced_model)
R2_reduced = 1 - summary_reduced_model$deviance / summary_reduced_model$null.deviance
R2_reduced
```

So the R2 score for the reduced model is 0.4563139.

Which is slightly less than the R2 score of the full model (which was 0.4565419), this is expected as our reduced model has fewer covariates than the full model.

\newpage

### Question 2

#### Fit a regression for whether the buyer had more than 20 percent down (onto everything but AMMORT and LPRICE).

```{r}
# Fit the logistic regression model excluding AMMORT and LPRICE
down_payment_model = glm(gt20dwn ~ . -AMMORT -LPRICE, family="binomial", data=homes)
```

#### Interpret effects for Pennsylvania state, 1st home buyers and the number of bathrooms.

```{r}
# Summary of the model to interpret coefficients
summary_down_payment_model = summary(down_payment_model)
summary_down_payment_model
```

```{r}
# Extract the relevant coefficients
coef(summary_down_payment_model)[c("(Intercept)", "STATEPA", "BATHS", "FRSTHOY"), ]
```

The coefficient of $\approx$ 0.601 for STATEPA suggests that being in the state of Pennsylvania increases the log-odds of having more than a 20% down payment by approximately 0.601 compared to the baseline.

The coefficient of $\approx$ 0.245 for BATHS means that each additional bathroom increases the log-odds of having a \> 20% down payment by approximately 0.245. By exponentiating the coefficient, $exp(0.245) \approx 1.277$, we can interpret this as meaning that each additional bathroom will need to a 27.7% increase in the odds of having a more than 20% down payment.

Finally the coefficient of $\approx$ -0.370 for FRSTHOY suggests that first home buyers have a lower log odds of having a \> 20% down payment than non-first home buyers by about 0.370.

Further, in the summary above, we notice that there are 3 stars (\*\*\*) for each of the three coefficients above, which suggests that the p values for these covariates are likely to be statistically significant.

#### Add and describe an interaction between 1st home-buyers and the number of baths.

```{r}
# - don't forget family="binomial"!
# - use +A*B in formula to add A interacting with B

# Fit the logistic regression model excluding AMMORT and LPRICE, and include an interaction term
interaction_model <- glm(gt20dwn ~ . -AMMORT -LPRICE + FRSTHO*BATHS, family="binomial", data=homes)

# Summary of the model to interpret coefficients
summary_interaction_model <- summary(interaction_model)

# Print the summary to interpret effects
summary_interaction_model
```

```{r}
# Extract the relevant coefficients
coef(summary_interaction_model)[c("(Intercept)", "STATEPA", "BATHS", "FRSTHOY", "BATHS:FRSTHOY"), ]
```

The negative coefficient for the interaction term (BATHS:FRSTHOY) suggests that for first-time home-buyers, each additional bathroom decreases the log odds of putting down more than 20% by 0.202 compared to buyers who are not purchasing their first home.\
This could indicate that first-time home buyers are either purchasing less expensive homes with more bathrooms or that the presence of additional bathrooms diminishes their ability or inclination to make larger down payments, perhaps due to the overall higher costs associated with homes that have more bathrooms.

\newpage

### Question 3

#### Focus only on a subset of homes worth $>100k$.

```{r}
# this is your training sample
subset_index = which(homes$VALUE>100000)
subset_homes = homes[subset_index,]
```

#### Train the full model from Question 1 on this subset.

```{r}
# Train the full model on this subset
full_model_subset = glm(log(LPRICE) ~ . - AMMORT, data=subset_homes)
```

#### Predict the left-out homes using this model.

```{r}
predicted_log_prices = predict(full_model_subset, newdata=homes[-subset_index,])
```

#### What is the out-of-sample fit (i.e. R2)?

```{r}
# Use the code ``deviance.R" to compute OOS deviance
source("deviance.R")

# Null model has just one mean parameter
ybar = mean(log(homes$LPRICE[-subset_index]))
ybar
D0 = deviance(y=log(homes$LPRICE[-subset_index]), pred=ybar)
D0
```

```{r}
# find the actual out of sample log prices
actual_log_prices = log(homes[-subset_index,]$LPRICE)

OOS_fit = R2(actual_log_prices, predicted_log_prices)
OOS_fit
```

So the out-of-sample fit is -0.04904513.

#### Explain why you get this value.  


Based on the output, we can see the value of r-squared is negative for the out-of-sample data. When the $R^2$ is negative, it typically indicates that the model's performance is worse than simply using the mean value as the prediction. In this case, it's likely that the model has over-fitted to the initial data, possibly due to the presence of outliers or due to the initial subset having different characteristics compared to the left out data. Additionally, if the model was trained only on homes worth \>100k, it may not generalize well to homes worth \<100k, leading to poor performance when applied to this subset of data.
